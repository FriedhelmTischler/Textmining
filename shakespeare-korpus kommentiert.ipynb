{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Der vollständige Shakespeare Korpus\n",
    "\n",
    "Wir bauen jetzt den Code aus dem Notebook \"preprocessing\" so um, dass wir auf dem gesamten Korpus arbeiten können.\n",
    "\n",
    "Erst mal die Initialisierung:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\fried\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "# NLTK laden und Stopwords holen\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ein einzelnes Stück\n",
    "\n",
    "Als nächstes lagern wir das Parsen eines Stückes in eine Funktion aus. Ausserdem optimieren wir wie von Ihnen vorgeschlagen, indem wir direkt die End-Tags suchen.\n",
    "\n",
    "Wir ändern die Struktur dabei etwas ab, so dass die Reihenfolge der Beiträge im Stück erhalten bleibt, siehe Folien und Kommentar in preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Strukurierung des oberen Dictionarys\n",
    "def work_name(filename):\n",
    "    return filename.split(\"\\\\\")[1][0:-4]\n",
    "#Bsp: Bei Cymbeline: [0] = \"comedies\" / [1] = \"Cymbeline\" .txt\n",
    "#     --> work_name = Cymbeline\n",
    "\n",
    "def category_name(filename):\n",
    "    return filename.split(\"\\\\\")[0]\n",
    "#Bsp: Bei Cymbeline: [0] = \"comedies\" / [1] = \"Cymbeline\" .txt\n",
    "#     --> category_name = comedies\n",
    "\n",
    "def parse_work(filename):\n",
    "    work = {\n",
    "        \"name\": work_name(filename),\n",
    "        \"category\": category_name(filename),\n",
    "        \"speakers\": [],\n",
    "        \"texts\": []\n",
    "    } #Einzelne .txt Dateien werden dem Dictionary zugeordnet.\n",
    "    \n",
    "    # Durchlauf 1: Speaker ermitteln\n",
    "    with open(filename, encoding = \"utf16\") as file:\n",
    "        text = file.read()\n",
    "        alle_tags = re.findall(\"</[^>]*>\", text)\n",
    "        \n",
    "        # Alle Tags für SCENE, ACT und STAGE \n",
    "        filtered = [a for a in alle_tags if not \"SCENE\" in a]\n",
    "        filtered = [a for a in filtered if not \"ACT\" in a]\n",
    "        filtered = [a for a in filtered if not \"STAGE\" in a]\n",
    "        filtered = [a for a in filtered if not \"SONG\" in a]\n",
    "        \n",
    "        # Alle Tags, die Kleinbuchstaben enthalten\n",
    "        filtered = [a for a in filtered if not any(c.islower() for c in a)]\n",
    "        \n",
    "        # Spitze Klammern entfernen:\n",
    "        filtered = [a[2:-1] for a in filtered]\n",
    "        for s in filtered:\n",
    "            if s not in work[\"speakers\"]:\n",
    "                work[\"speakers\"].append(s)\n",
    "    \n",
    "    with open(filename, encoding=\"utf16\") as f:\n",
    "        current_speaker=\"\"\n",
    "        current_text=\"\"\n",
    "        stage=False\n",
    "        for line in f:\n",
    "            if current_speaker==\"\":\n",
    "                for key in work[\"speakers\"]:\n",
    "                    if \"<\" + key + \">\" in line:\n",
    "                        current_speaker=key\n",
    "                        break\n",
    "            else:\n",
    "                if current_speaker!=\"\" and \"</\" + current_speaker + \">\" in line:\n",
    "                            work[\"texts\"].append({\n",
    "                                \"speaker\": current_speaker,\n",
    "                                \"text\": current_text.strip()\n",
    "                            })\n",
    "                            current_text=\"\"\n",
    "                            current_speaker=\"\"                \n",
    "                if current_speaker!=\"\":\n",
    "                    if \"<STAGE\" in line:\n",
    "                        stage=True\n",
    "                    if \"</STAGE\" in line:\n",
    "                        stage=False\n",
    "                    if stage==False:\n",
    "                        # ggf. noch STAGE-ende-tag wegwerfen\n",
    "                        line = line.replace(\"<SONG>\", \"\").replace(\"</SONG>\", \"\")\n",
    "                        current_text=current_text + \" \" + re.sub(r'.*</STAGE DIR>','',line).strip()\n",
    "    return work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alle Stücke\n",
    "\n",
    "Jetzt gehen wir alle Dateien durch und erstellen eine Liste aller Stücke."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Midsummer-Night's Dream (comedies)\n",
      "All’s Well that Ends Well (comedies)\n",
      "As You Like It (comedies)\n",
      "Cymbeline (comedies)\n",
      "Love's Labour's Lost (comedies)\n",
      "Measure for Measure (comedies)\n",
      "Much Ado About Nothing (comedies)\n",
      "Pericles, Prince of Tyre (comedies)\n",
      "The Comedy of Errors (comedies)\n",
      "The Merchant Of Venice (comedies)\n",
      "The Merry Wives of Windsor (comedies)\n",
      "The Taming of the Shrew (comedies)\n",
      "The Tempest (comedies)\n",
      "The Two Gentlemen of Verona (comedies)\n",
      "The Winter’s Tale (comedies)\n",
      "Troilus and Cressida (comedies)\n",
      "Twelfth-Night; or What You Will (comedies)\n",
      "The Famous History of the Life of King Henry VIII (historical)\n",
      "The First Part of King Henry IV (historical)\n",
      "The First Part of King Henry VI (historical)\n",
      "The Life and Death of King John (historical)\n",
      "The Life of King Henry V (historical)\n",
      "The Second Part of King Henry IV (historical)\n",
      "The Second Part of King Henry VI (historical)\n",
      "The Third Part of King Henry VI (historical)\n",
      "The Tragedy of King Richard II (historical)\n",
      "The Tragedy of King Richard III (historical)\n",
      "Anthony and Cleopatra (tragedies)\n",
      "Coriolanus (tragedies)\n",
      "Hamlet, Prince of Denmark (tragedies)\n",
      "Julius Caesar (tragedies)\n",
      "King Lear (tragedies)\n",
      "Macbeth (tragedies)\n",
      "Othello, the Moor of Venice (tragedies)\n",
      "Romeo And Juliet (tragedies)\n",
      "Timon of Athens (tragedies)\n",
      "Titus Andronicus (tragedies)\n"
     ]
    }
   ],
   "source": [
    "works = []\n",
    "for filename in Path(\".\").glob('*\\\\*.txt'):\n",
    "    w = parse_work(str(filename))\n",
    "    print(\"{} ({})\".format(w[\"name\"], w[\"category\"]))\n",
    "    works.append(w)\n",
    "    \n",
    "#def parse_work(filename):\n",
    "#    work = {\n",
    "#        \"name\": work_name(filename),\n",
    "#        \"category\": category_name(filename),\n",
    "#        \"speakers\": [],\n",
    "#        \"texts\": []\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenisierung\n",
    "\n",
    "Hier ist die tokenize-Funktion, die wir entwickelt haben:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokens = text.split()\n",
    "    tokens = [t.strip(\".,;!?\").lower() for t in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hilfsfunktionen\n",
    "\n",
    "Damit wir wieder unsere Analysen durchführen können, sollten wir ein paar Hilfsfunktionen entwickeln:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def work(name):\n",
    "    for w in works:\n",
    "        if w[\"name\"] == name:\n",
    "            return w\n",
    "    raise ValueError(\"Work not found: {}\".format(name))\n",
    "\n",
    "def work_text(name):\n",
    "    alleszusammen = \"\"\n",
    "    w = work(name)\n",
    "    for t in w[\"texts\"]:\n",
    "        alleszusammen = alleszusammen + \" \" + t[\"text\"]\n",
    "    return alleszusammen\n",
    "\n",
    "def speaker_text(work, speaker):\n",
    "    alleszusammen = \"\"\n",
    "    for t in work[\"texts\"]:\n",
    "        if t[\"speaker\"] == speaker:\n",
    "            alleszusammen = alleszusammen + \" \" + t[\"text\"]\n",
    "    return alleszusammen\n",
    "\n",
    "# Wir testen, ob das auch den alten Werten entspricht, vgl. preprocessing\n",
    "\n",
    "tokens = tokenize(work_text(\"Cymbeline\"))\n",
    "print(\"Anzahl Tokens: {}\".format(len(tokens)))\n",
    "print(\"Anzahl unterschiedlicher Tokens: {}\".format(len(set(tokens))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords\n",
    "\n",
    "Hier die Entfernung der Stopwords, wie in preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokens):\n",
    "    return [t for t in tokens if t not in stop]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hausaufgaben"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Längstes Stück\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in sorted(works, reverse=True, key=lambda w: len(tokenize(work_text(w[\"name\"])))):\n",
    "    print(\"{} ({} Tokens)\".format(w[\"name\"], len(tokenize(work_text(w[\"name\"])))))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sprecher mit dem meisten Text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_speakers = [(work[\"name\"], speaker) for work in works for speaker in work[\"speakers\"]]\n",
    "for s in sorted(all_speakers, reverse=True, key=lambda s: len(tokenize(speaker_text(work(s[0]),s[1])))):\n",
    "    print(\"{} in {} ({} Tokens)\".format(s[1], s[0], len(tokenize(speaker_text(work(s[0]),s[1])))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Am ähnlichsten zu Hamlet\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
